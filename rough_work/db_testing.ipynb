{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9635b68d-375e-4335-90e4-f469cc2f488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/A118390615/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool\")\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import JSON, Text\n",
    "#from sqlalchemy.dialects.postgresql import JSON \n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from db_operations.utility_db import process_individual_resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c9b5fe-6019-4988-aa0f-2c9857efc674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f96924",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Resume_Ketan.pdf\", \"rb\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30892e60-d0e7-47b2-8224-8bc1957bb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobRole = \"Data Scientist\"\n",
    "resume_string_1 = \"\"\"\n",
    "ketan.kishore31@gmail.c  om  +91 7488391342  House No. 4098, Sector   –  4/F, Bokaro Steel City,  Jharkhand   -   827004  EDUCATION  10th from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2010 with  95% GPA  12 th   from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2013 with  82% GPA  B .Tech. in Electronics and  Communication from SRM  University, Chennai in 2017  with 76.85 GPA  M.Tech in Data Science and  Engineering from BITS   -  Work Integrated, 2023 with  7.27 CGPA  Ketan Kishore  An enthusiastic & high energy driven professional ,   targeting challenging assignments as a   Data  Scientist   with an organization of high repute .  GitHub Repo:   https://github.com/ketankishore27  Docker Hub Repo:   https://hub.docker.com/u/ketankishore27  LinkedIn :   https://www.linkedin.com/in/ketan - kishore - b89643150/  PROFILE SUMMARY  •   Goal - oriented professional with experience in   Insurance ,   Banking , and   Telecom   domains.  •   Skilled in   Predictive Modeling   using   Supervised   and   Unsupervised   Learning  •   E xperienced in   Text   Analytics   using   NLP , LLM   and   Generative AI  •   Exposure   to   Cloud services like   AWS   and   Azure  •   Proficient   in working with   Distributed Framework   for   scalable   Analytics and Modelling  PROJECTS  T - Systems India Pvt Ltd   Apr’21   –   Present  Client:   Deutsche Telekom (Germany)  Common Service Desk   (Generative AI)  •   Partnered with stakeholders to define clear project goals and success criteria .  •   Designed and implemented backend models for service desk   support   using RASA and RAG .  •   Fine - tuned model responses to enhance accuracy and   user experience .  •   Created   APIs   for real - time AI model integration .  •   Analysis for incorrect responses from the chatbot.  Client:   Deutsche   Telekom   ( Germany,   Croatia , Hungary , Poland )  Router Domain   (Machine Learning, Big Data)  •   Feature store for telecom data .  •   Developed   models to predict   issues   like   Device, I nstallation , Line, Wi - Fi   error etc.  •   D ashboards   for the bootstrapped new router   developed by   organization .  •   Created flows to   troubleshoot and find new issues in new router versions.  •   Developed customer profiling and journey analysis for people visiting our app.  •   Helped in   finding out issues in the Deutsche Telecom `My Magenta` app  Bajaj Finance   Jun ’ 20   –   Apr’21  Client: Loans /Lending   Team  Money Manager   (Machine Learning)  •   Created ML model to   classify   transactional message   and store entities  •   Assisting   in creating real time offer generation pipeline based on above extracted entities  Client: E - Store   Team  Nearest Dealer Solution   (Machine Learning)  •   Created a   ML model to identify   nearest dealers   for   visitors   on the website.  •   Incorporated business rules to recommend dealers based on loyalty, reviews/score and distance  Client: Marketing   + Cards Team  Clickstream Analytics   (Adobe Analytics, Big Data)  •   Created reports on   Adobe Analytics   on Customer Journey, Path, Churn and Anomaly  •   A nalyze and   r ecommend concordan t /discordan t   simulations sent to the identified visitors\n",
    "PERSONAL DETAILS  Date of Birth: 20 th   February 1995  Lan guages Known: English and Hindi  HOBBIES  •   Travel  •   Cook  •   Gym  •   Personal Projects ( Here )  •   Friends Catchup  Capgemini.   Oct ’ 17   -   Jun ’20  Client:   Swiss Re  Trip Optimization   -   POC (Tableau, Statistical Modelling)  •   Developed a constrained   algorithm   to optimize trip allocation   cost .  •   Implemented hierarchical de - allocation of trips consi dering corporate band and real time  Tableau frontend filters  •   Created and presented   dashboard having   Drill Down Reports   and overall summary  Client:   Sunlife Financials  News - Feed   –   POC   (N atural   L anguage   P rocessing , Web Scraping)  •   Developed a web crawler   to search   the web   and perform NLP task to create insights.  •   Worked   on possibility to use   satellite imagery to predict Catastrophes damage index .  Client:   Assurant Employee Benefits  Analytics   and Automation   (Python, Basic Machine Learning)  •   Analyze ,   Visualize,   and deduce KPI based on claims distributed over geographical area.  •   Implemented ML and DL model to predict   Fraudulent/Incorrect C laims  •   Work ed   parallel   on   development of   Churn Model   ( POC ).  •   Created   Python API’s and Selenium automations\n",
    "\"\"\"\n",
    "\n",
    "data = {\n",
    "    \"jobRole\": jobRole,\n",
    "    \"resumeText\": resume_string_1,\n",
    "    \"resumeText_Binary\": psycopg2.Binary(file),\n",
    "    \"resume_name\": \"Resume_Ketan\",\n",
    "    \"resume_mimetype\": \"application/pdf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8606e008",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Binary is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m return_payload = {}\n\u001b[32m      6\u001b[39m headers = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# If authentication is required\u001b[39;00m\n\u001b[32m      9\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m get_contact_information = {\u001b[33m\"\u001b[39m\u001b[33mgetContacts\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://127.0.0.1:8000/getContacts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m.json()}\n\u001b[32m     12\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33memail_id\u001b[39m\u001b[33m\"\u001b[39m] = get_contact_information.get(\u001b[33m\"\u001b[39m\u001b[33mgetContacts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m).get(\u001b[33m\"\u001b[39m\u001b[33memail_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     13\u001b[39m return_payload[\u001b[33m\"\u001b[39m\u001b[33memail_id\u001b[39m\u001b[33m\"\u001b[39m] = get_contact_information.get(\u001b[33m\"\u001b[39m\u001b[33mgetContacts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m).get(\u001b[33m\"\u001b[39m\u001b[33memail_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/sessions.py:575\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[32m    563\u001b[39m req = Request(\n\u001b[32m    564\u001b[39m     method=method.upper(),\n\u001b[32m    565\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m     hooks=hooks,\n\u001b[32m    574\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m prep = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m proxies = proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    579\u001b[39m settings = \u001b[38;5;28mself\u001b[39m.merge_environment_settings(\n\u001b[32m    580\u001b[39m     prep.url, proxies, stream, verify, cert\n\u001b[32m    581\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/sessions.py:484\u001b[39m, in \u001b[36mSession.prepare_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    481\u001b[39m     auth = get_netrc_auth(request.url)\n\u001b[32m    483\u001b[39m p = PreparedRequest()\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/models.py:370\u001b[39m, in \u001b[36mPreparedRequest.prepare\u001b[39m\u001b[34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_headers(headers)\n\u001b[32m    369\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_cookies(cookies)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mself\u001b[39m.prepare_auth(auth, url)\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Note that prepare_auth must be last to enable authentication schemes\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# such as OAuth to work on a fully prepared request.\u001b[39;00m\n\u001b[32m    375\u001b[39m \n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# This MUST go after prepare_auth. Authenticators could add a hook\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/requests/models.py:510\u001b[39m, in \u001b[36mPreparedRequest.prepare_body\u001b[39m\u001b[34m(self, data, files, json)\u001b[39m\n\u001b[32m    507\u001b[39m content_type = \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m     body = \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidJSONError(ve, request=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py:200\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py:258\u001b[39m, in \u001b[36mJSONEncoder.iterencode\u001b[39m\u001b[34m(self, o, _one_shot)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    254\u001b[39m     _iterencode = _make_iterencode(\n\u001b[32m    255\u001b[39m         markers, \u001b[38;5;28mself\u001b[39m.default, _encoder, \u001b[38;5;28mself\u001b[39m.indent, floatstr,\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_separator, \u001b[38;5;28mself\u001b[39m.item_separator, \u001b[38;5;28mself\u001b[39m.sort_keys,\n\u001b[32m    257\u001b[39m         \u001b[38;5;28mself\u001b[39m.skipkeys, _one_shot)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type Binary is not JSON serializable"
     ]
    }
   ],
   "source": [
    "#def process_individual_resume(data: dict):\n",
    "\n",
    "final_payload = {}\n",
    "return_payload = {}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": None # If authentication is required\n",
    "}\n",
    "\n",
    "get_contact_information = {\"getContacts\": requests.post(\"http://127.0.0.1:8000/getContacts\", json = data, headers=headers).json()}\n",
    "data[\"email_id\"] = get_contact_information.get(\"getContacts\", None).get(\"email_id\", None)\n",
    "return_payload[\"email_id\"] = get_contact_information.get(\"getContacts\", None).get(\"email_id\", None)\n",
    "return_payload[\"contact_number\"] = get_contact_information.get(\"getContacts\", None).get(\"mobile_number\", None)\n",
    "get_name = requests.post(\"http://127.0.0.1:8000/getNames\", json = data, headers=headers).json()\n",
    "return_payload[\"name\"] = get_name.get(\"name\", None)\n",
    "get_custom_scores = {\"getCustomScores\": requests.post(\"http://127.0.0.1:8000/getCustomScores\", json = data, headers=headers).json()}\n",
    "get_summary_overview = {\"getSummaryOverview\": requests.post(\"http://127.0.0.1:8000/getSummaryOverview\", json = data, headers=headers).json()}\n",
    "return_payload[\"summary_overview\"] = get_summary_overview.get(\"getSummaryOverview\", None).get(\"comment\", None)\n",
    "get_functional_constituent = {\"getFunctionalConstituent\": requests.post(\"http://127.0.0.1:8000/getFunctionalConstituent\", json = data, headers=headers).json()}\n",
    "get_other_comments = {\"getOtherComments\": requests.post(\"http://127.0.0.1:8000/getOtherComments\", json = data, headers=headers).json()}\n",
    "get_education = {\"getEducation\": requests.post(\"http://127.0.0.1:8000/getEducation\", json = data, headers=headers).json()}\n",
    "get_score_resume = {\"scoreResume\": requests.post(\"http://127.0.0.1:8000/scoreResume\", json = data, headers=headers).json()}\n",
    "return_payload[\"score_resume\"] = get_score_resume.get(\"scoreResume\", None)\n",
    "get_technical_constituent = {\"getTechnicalConstituent\": requests.post(\"http://127.0.0.1:8000/getTechnicalConstituent\", json = data, headers=headers).json()}\n",
    "get_comapny = {\"getCompany\": requests.post(\"http://127.0.0.1:8000/getCompany\", json = data, headers=headers).json()}\n",
    "get_project = {\"getProjects\": requests.post(\"http://127.0.0.1:8000/getProjects\", json = data, headers=headers).json()}\n",
    "get_data = {\"job_role\": data.get('jobRole', None), \"resume_text\": data.get('resumeText', None)}\n",
    "exp_params = requests.post(\"http://127.0.0.1:8000/getYoe\", json = data, headers=headers).json()\n",
    "get_yoe = {'getYoe': exp_params.get(\"yoe\", None)}\n",
    "get_ryoe = {'getRyoe': exp_params.get(\"ryoe\", None)}\n",
    "get_location = {\"getLocation\": requests.post(\"http://127.0.0.1:8000/getLocation\", json = data, headers=headers).json()}\n",
    "get_recruiters_overview = {\"getRecruitersOverview\": requests.post(\"http://127.0.0.1:8000/getRecruitersOverview\", json = data, headers=headers).json()}\n",
    "get_designation = {\"getDesignation\": requests.post(\"http://127.0.0.1:8000/getDesignation\", json = data, headers=headers).json()}\n",
    "input_data = {\"input_data\": {**get_name, **get_data}}\n",
    "get_mode = {\"mode\": \"batch\"}\n",
    "final_payload = {**input_data, **get_contact_information, **get_custom_scores, **get_summary_overview, **get_functional_constituent, **get_other_comments, \n",
    "                    **get_education, **get_score_resume, **get_technical_constituent, **get_comapny, **get_project, **get_yoe, **get_ryoe, **get_recruiters_overview, \n",
    "                    **get_designation, **get_location, **get_mode, \"resume\": data.get('resumeText_Binary', None), \n",
    "                    \"resume_filename\": data.get('resume_name', None), \"resume_mimetype\": data.get('resume_mimetype', None)}\n",
    "\n",
    "status = requests.post(\"http://127.0.0.1:8000/assembleData\", json = final_payload, headers=headers)\n",
    "return_payload[\"parsed_status\"] = \"Successful\"\n",
    "\n",
    "#    return return_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b9b2f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psycopg2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_individual_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mprocess_individual_resume\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     35\u001b[39m input_data = {\u001b[33m\"\u001b[39m\u001b[33minput_data\u001b[39m\u001b[33m\"\u001b[39m: {**get_name, **get_data}}\n\u001b[32m     36\u001b[39m get_mode = {\u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     37\u001b[39m final_payload = {**input_data, **get_contact_information, **get_custom_scores, **get_summary_overview, **get_functional_constituent, **get_other_comments, \n\u001b[32m     38\u001b[39m                  **get_education, **get_score_resume, **get_technical_constituent, **get_comapny, **get_project, **get_yoe, **get_ryoe, **get_recruiters_overview, \n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m                  **get_designation, **get_location, **get_mode, \u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mpsycopg2\u001b[49m.Binary(data.get(\u001b[33m'\u001b[39m\u001b[33mresumeText_Binary\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)), \n\u001b[32m     40\u001b[39m                  \u001b[33m\"\u001b[39m\u001b[33mresume_filename\u001b[39m\u001b[33m\"\u001b[39m: data.get(\u001b[33m'\u001b[39m\u001b[33mresume_name\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[33m\"\u001b[39m\u001b[33mresume_mimetype\u001b[39m\u001b[33m\"\u001b[39m: data.get(\u001b[33m'\u001b[39m\u001b[33mresume_mimetype\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)}\n\u001b[32m     42\u001b[39m status = requests.post(\u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:8000/assembleData\u001b[39m\u001b[33m\"\u001b[39m, json = final_payload, headers=headers)\n\u001b[32m     43\u001b[39m return_payload[\u001b[33m\"\u001b[39m\u001b[33mparsed_status\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mSuccessful\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'psycopg2' is not defined"
     ]
    }
   ],
   "source": [
    "process_individual_resume(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f11ef7-0e18-4af7-9539-f6fd94c3f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_request = {'fileName': 'Resume Ravi Prabhakar.pdf', 'fileSize': 773292, 'resumeText': 'RAVI PRABHAKAR  Project Manager (AGM, Automation Process Management)  +91 - 9920036707/9321236707  ravi.pbkr@gmail.com  Pursue opportunities   as a Digital Transformation Leader, bringing expertise in spearheading innovative initiatives in Network  Operations & Robotic Process Automation (RPA) while improving business processes & operational efficiency across various sect ors.  PROFILE SUMMARY  \\uf0b7   Strategic Digital Transformation Leader   with   over   17 years   of experience   driving innovation and leading end - to - end  automation initiatives, optimizing network operations, and transforming RPA workflows to enhance operational   efficiency and  deliver substantial cost savings across large - scale global projects.  \\uf0b7   Project Management Expertise   in managing complex digital transformations, delive ring cloud - based solutions (AWS)   and  orchestrating the end - to - end project lifecycle from pla nning to execution, ensuring seamless integration, operational excellence,  and client satisfaction while driving cost - effective solutions.  \\uf0b7   Stakeholder Engagement & Leadership:   Adept at managing cross - functional teams, aligning stakeholders across diverse  g eographies, and leveraging strategic thinking to mitigate project risks, resolve issues proactively, and maintain alignment w ith  business objectives and client expectations.  \\uf0b7   Expertise in   service delivery excellence   through continuous process audit, quality   assurance, and compliance management  (ISO 27000/20000), ensuring high - quality deliverables in alignment with SLAs and business standards, while overseeing project  schedules, budgets, and change management.  \\uf0b7   Data - Driven Decision Maker , leveraging deep insig hts from network tools (VFO, ArcGIS, TEOCO) and cloud technologies to  identify cost - saving opportunities   &   optimize operational performance, ensuring the sustainability   &   growth of client businesses.  \\uf0b7   Trained   PMP and   Certified   Scrum Master , with expertise i n agile methodologies and DevOps practices, empowering high -  performance teams to deliver impactful results, accelerate time - to - market,   &   maximize business value across the telecom   &   IT  services sectors.  CORE COMPETENCIES  \\uf0b7   Digital Transformation  \\uf0b7   Project   Management  \\uf0b7   Process Optimization  \\uf0b7   Stakeholder Engagement  \\uf0b7   Risk Management Framework  \\uf0b7   Quality Assurance Protocols  \\uf0b7   Cloud Solutions  \\uf0b7   Change Management  \\uf0b7   Service Delivery Excellence  \\uf0b7   Project Lifecycle Management  \\uf0b7   Operational Efficiency Metrics  \\uf0b7   RPA Automation  \\uf0b7   Client   Relationship Management  \\uf0b7   Network Operations  EDUCATION  \\uf0b7   B.E. in Electronics and Telecommunication Engineering   from Mumbai University | 2007  CERTIFICATION S  \\uf0b7   ITIL V3   ||   Prince2 Practitioner   ||   CSM (Certified Scrum Master)   ||   DevOps Engineer  TECHNICAL SKILLS  \\uf0b7   Project Management:  \\uf0b7   Cloud & Automation:  \\uf0b7   OSS Tools:  \\uf0b7   BSS Tools:  \\uf0b7   Design & Network Tools:  :   PMP Trained, Microsoft Project  :   AWS, Python, RPA (AA, Blue prism)  :   VFO, Kleiman, ADW, Clarity, ArcGIS, EFMS  :   TEOCO, Granite, ATOS, BMP, ASOC, USRP  :   MS Visio, Clarity, ArcGIS  SOFT SKILLS  \\uf0b7   Leadership & Team Management || Decision - Making || Communication & Interpersonal Skills || Collaboration & Cross - Functional  Coordination || Adaptability & Resilience || Attention to Detail || Strategic   Communication\\nWORK EXPERIENCE  Nov’21   –   Present | Project Manager (AGM, Automation Process Management) | Vodafone Idea Ltd.  Key Result Areas:  \\uf0b7   Driving Process Automation:   Leading 200+ automated processes, saving 100+ FTEs and 200 man - hours, with 30   more projects  expected to save 50+ FTEs.  \\uf0b7   Managing Projects:   Overseeing projects, integrating resources with expertise for successful delivery.  \\uf0b7   Aligning Stakeholders & Mitigating Risks:   Managing scope, risks,   &   conflicts while ensuring stakeholder alignment   &  preventing delays.  \\uf0b7   Leading High - Performing Teams:   Building an d guiding teams, defining roles   &   ensuring accountability for seamless execution.  \\uf0b7   Budget & Timeline Oversight:   Monitoring budgets   &   schedules, implementing corrective actions to ensure timely, cost - effective  delivery.  \\uf0b7   Tracking & Reporting Progress:   Acting as the primary contact for pr oject updates, addressing risks   &   ensuring smooth execution.  \\uf0b7   Risk Forecasting & Issue Resolution:   Proa ctively identifying risks, mitigating challenges, and keeping projects on track.  \\uf0b7   Project Scheduling & Planning:   Developing   &   maintaining structured project schedules   &   comprehensive plans for successful  execution.  \\uf0b7   Stakeholder Communication & Status Reporti ng:   Providing clear updates, adapting strategies to evolving requirements, and  ensuring transparency.  \\uf0b7   Change & Quality Management:   Managing scope & resource changes while ensuring quality control for high - standard deliverables.  \\uf0b7   Cross - Functional Collaborati on:   Encouraging teamwork and leveraging diverse expertise to drive innovation and efficiency.  \\uf0b7   Post - Project Evaluation:   Conducting reviews, sharing insights, and improving future project execution.  \\uf0b7   Audit & Compliance Oversight:   Leading ISO 27000 & ISO 20000   audits, ensuring adherence to industry standards.  Jul’14   –   Oct’21 | Project Manager | Accenture Solutions (Region   –   USA - Remote)  Key Result Areas:  \\uf0b7   Developed Service Portfolio:   Created a comprehensive cloud, network, and Ethernet service portfolio for 50+ IT enterprises.  \\uf0b7   Achieved Cost Savings & Managed SLA:   Secured $500K – $1M in savings by managing SLA penalties, enhancing client profitability.  \\uf0b7   Led Team & Managed Project Lifecycl e:   Led a team, overseeing the full project lifecycle from planning to deployment.  \\uf0b7   Client Collaboration & Cost Optimization:   Analyzed client requirements and identified cost - saving opportunities aligned with  business goals.  \\uf0b7   Delivered Customized Solutions:   Designed and deployed tailored product solutions, improving service offerings.  \\uf0b7   Cultivated Client Relationships:   Strengthened client relationships, minimizing escalations and driving business value.  \\uf0b7   Refined Business Models:   Provided data - driven insights to   optimize business models and improve product performance.  \\uf0b7   Provided Transparent Reporting:   Delivered consistent status updates to both on - site and offshore leadership.  \\uf0b7   Developed Cost - Saving Strategies:   Identified and implemented cost - saving opportunities t hrough data analysis.  \\uf0b7   Ensured Client Satisfaction:   Fostered strong client relationships by addressing needs and ensuring high satisfaction.  PREVIOUS EXPERIENCE  Apr’13   –   Jun’14 | Deputy Manager | Alcatel Lucent Managed Solutions  Feb’11   –   Mar’13 |   Deputy Manager | Reliance Communication  Jul’09   –   Jan’11 | SDH   Engineer   | Tulip Telecom  Jan’08   –   Jun’09 | Trainee Engineer | Ericsson India (Off - role)', 'pageCount': 2, 'jobRole': 'Project Manager - 3', 'processIndex': 2, 'totalFiles': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b46710-2921-4184-84e9-63400a4805f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_extraction(wordList = None, jobRole = None, experience = None, jobDescription = None):\n",
    "\n",
    "    print(\"WordList: \", wordList)\n",
    "    print(\"JobRole: \", jobRole)\n",
    "    print(\"Experience: \", experience)\n",
    "    print(\"jobDescription: \", jobDescription)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a206bd-1eaf-471a-9d9f-75300d39038e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe92441-91cd-4b96-bcaf-929f8f07f275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ad20b-b27a-498a-8b0a-52a609125f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953cdec-e8ef-43cb-ae2b-c81b99eaed17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812bb9f1-f5b6-438d-abe6-c015dc3fe9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fileName', 'fileSize', 'resumeText', 'pageCount', 'jobRole', 'processIndex', 'totalFiles'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_request.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f31450-1557-4c65-9ee6-c4272800ae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce580427-84f1-42cc-a4ee-be3a60689ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"postgresql+psycopg2://postgres:resume_db@localhost:5432/postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8efe577a-9623-4597-9dd1-c2df88f97f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data: dict):\n",
    "\n",
    "    TABLE_NAME = os.getenv(\"TABLE_NAME\", None)\n",
    "    email_id = data.get(\"email_id\", None)\n",
    "    sql_query = f\"select * from {TABLE_NAME} where email_id = '{email_id}'\"\n",
    "    data = pd.read_sql(sql_query, engine).to_dict(\"records\")[0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53294845-2c66-4265-8fe3-42202f62a342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2889a73b-90bf-4499-abb4-ce6f2d6b7a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['candidate_id', 'name', 'job_role', 'resume_raw_text', 'email_id', 'mobile_number', 'score_resume', 'get_contacts', 'get_summary_overview', 'get_custom_scores', 'get_other_comments', 'get_functional_constituent', 'get_technical_constituent', 'get_education', 'get_projects', 'get_company', 'mode'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_data({\"email_id\": \"ketan.kishore31@gmail.com\"}).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d5c2b2-687e-48ce-84fd-b8c14778af16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c765fee-f722-49bf-a9a2-f610759b9754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a678bda3-0f02-4e91-88e6-ecfcddb39cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_individual_resume(data: dict):\n",
    "\n",
    "    final_payload = {}\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": None # If authentication is required\n",
    "    }\n",
    "    \n",
    "    get_contact_information = {\"getContacts\": requests.post(\"http://127.0.0.1:8000/getContacts\", json = data, headers=headers).json()}\n",
    "    data[\"email_id\"] = get_contact_information.get(\"getContacts\", None).get(\"email_id\", None)\n",
    "    get_name = requests.post(\"http://127.0.0.1:8000/getNames\", json = data, headers=headers).json()\n",
    "    get_custom_scores = {\"getCustomScores\": requests.post(\"http://127.0.0.1:8000/getCustomScores\", json = data, headers=headers).json()}\n",
    "    get_summary_overview = {\"getSummaryOverview\": requests.post(\"http://127.0.0.1:8000/getSummaryOverview\", json = data, headers=headers).json()}\n",
    "    get_functional_constituent = {\"getFunctionalConstituent\": requests.post(\"http://127.0.0.1:8000/getFunctionalConstituent\", json = data, headers=headers).json()}\n",
    "    get_other_comments = {\"getOtherComments\": requests.post(\"http://127.0.0.1:8000/getOtherComments\", json = data, headers=headers).json()}\n",
    "    get_education = {\"getEducation\": requests.post(\"http://127.0.0.1:8000/getEducation\", json = data, headers=headers).json()}\n",
    "    get_score_resume = {\"scoreResume\": requests.post(\"http://127.0.0.1:8000/scoreResume\", json = data, headers=headers).json()}\n",
    "    get_technical_constituent = {\"getTechnicalConstituent\": requests.post(\"http://127.0.0.1:8000/getTechnicalConstituent\", json = data, headers=headers).json()}\n",
    "    get_comapny = {\"getCompany\": requests.post(\"http://127.0.0.1:8000/getCompany\", json = data, headers=headers).json()}\n",
    "    get_project = {\"getProjects\": requests.post(\"http://127.0.0.1:8000/getProjects\", json = data, headers=headers).json()}\n",
    "    get_data = {\"job_role\": data.get('jobRole', None), \"resume_text\": data.get('resumeText', None)}\n",
    "    input_data = {\"input_data\": {**get_name, **get_data}}\n",
    "    get_mode = {\"mode\": \"batch\"}\n",
    "    final_payload = {**input_data, **get_contact_information, **get_custom_scores, **get_summary_overview, **get_functional_constituent, **get_other_comments, \n",
    "                     **get_education, **get_score_resume, **get_technical_constituent, **get_comapny, **get_project, **get_mode}\n",
    "\n",
    "    status = requests.post(\"http://127.0.0.1:8000/assembleData\", json = final_payload, headers=headers)\n",
    "    \n",
    "    return final_payload, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b977ba49-a6a2-4639-8075-b87d9b67f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_obj, status = process_individual_resume(sample_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "62685bd4-abe6-414c-9d57-f29b8b48f70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_data', 'getContacts', 'getCustomScores', 'getSummaryOverview', 'getFunctionalConstituent', 'getOtherComments', 'getEducation', 'scoreResume', 'getTechnicalConstituent', 'getCompany', 'getProjects', 'mode'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_obj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4165e8-b8f6-43d7-81a2-76614d130e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4ded14a-7da0-4954-b372-ce03a1061d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = process_individual_resume(sample_request).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc443e52-b00d-4dfe-ae43-fa786d812033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email_id': 'ravi.pbkr@gmail.com',\n",
       " 'mobile_number': '+91 - 9920036707/9321236707',\n",
       " 'color': 'green',\n",
       " 'comment': 'Both contact number and email ID are present.'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b791c151-3c99-433f-a08f-108425a380d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 1, 'y': 2, 'a1': 3, 'z': 4}\n"
     ]
    }
   ],
   "source": [
    "dict_a = {'x': 1, 'y': 2}\n",
    "dict_b = {'a1': 3, 'z': 4}\n",
    "merged_dict = {**dict_a, **dict_b}\n",
    "print(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f2c42-39a7-483d-a27c-2a8f27fda921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53c28197-49ef-4c75-b2da-6e46eabaec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harsh Kumar  *   Bangalore, KA      +91 9142780129   #   harshsingh94636@gmail.com   ï   harshkumar   ´   Document Repository  Education  M.S. Ramaiah Institute of Technology   Bangalore, KA  Bachelor of Engineering   •   CGPA: 7.58   Dec 2022 – Sep 2026  Delhi Public School   Bokaro, JH  Senior Secondary (Class XII)   •   88.8%   Jul 2020 – Jul 2022  DAV Public School   Ranchi, JH  Secondary Education (Class X)   •   90.8%   April 2019 – June 2020  Relevant Coursework  •   Data Cleaning  •   EDA  •   SQL for Data Extraction  •   Data Visualization  •   BI Reporting  •   Statistical Testing  •   Predictive Analytics  •   Python for Data Analysis  Experience  House Sutra Llp   December 2024 – May 2025  Data Analyst Intern   Jaipur, RJ  •   Led   analysis of large customer data sets   to extract insights, directly influencing business strategy and improving conversion rates by 20%.  •   Designed and automated analytical dashboards   (Excel, SQL, Python, Power BI)   to track key operational metrics, reducing manual reporting time by 30%.  •   Managed and tracked multiple initiatives, creating analytical reports and progress dashboards for leadership review.  Tata Steel Ltd.   September 2024 – October 2024  Data Analyst Intern   Jamshedpur, JH  •   Successfully conducted data cleaning and feature extraction,including parsing motor descriptions and normalizing key performance metrics.  •   Developed predictive maintenance strategies using SAP data,reducing downtime and maintenance costs.  •   Identified performance trends and anomalies through temperature and pressure variation analysis using advanced statistical methods.  Projects  Sales Data Visualization and Automation   Tools: Power BI, Excel, Python  •   Streamlined sales data analysis by first identifying key trends and top-performing products in Excel, then automating data cleaning and summarization using   Python (Pandas, NumPy, Matplotlib)   to ensure accurate insights.  •   Developed interactive   Power BI dashboards   from the processed data, enabling real-time sales performance monitoring for House Sutra’s leadership and reducing manual reporting time by 35%.  Areas of Expertise  •   Technical Skills:   Advanced SQL, Python (Pandas, Numpy, Matplotlib), Advanced Excel, Power BI, Tableau.  •   Tools & Platforms:   MySQL, PyCharm, Microsoft Office Suite, VScode.  •   Soft Skills:   Cross-functional teamwork, Analytical Thinking, Problem Solving, Strong verbal & written communication, Exceptional organisational skills, Interpersonal skills.  Research / Publications  Innovative Facility Layout   [Presented: IPDIMS on 4th and 5th December,2024 @ NIT Rourkela]  Co-Author   Ramaiah Institute of Technology  •   Analyzed industrial workflows through site visits, documenting processes and mapping interdepartmental relationships. Translated qualitative ratings (A, E, I, O, U, X) into quantitative values (6–1) for optimized layout analysis.  Certifications / Achievements  1.   Google Data Analytics – Google : Practical skills in SQL,   spreadsheets, Tableau, R, Data Preparation, Data Wrangling, Root Cause Analysis, Business Intelligence Reporting, . 2. Python For Everybody – University of Michigan: Python scripting, data transformation. 3. SQL 50 – LeetCode: Hands-on practice with complex SQL queries, filtering, aggregation, Advanced joins. 4. Techno Vision Case Study: Secured 1st position at Techno 2025   awarded Rs 8,000 cash prize for outstanding performance.\n"
     ]
    }
   ],
   "source": [
    "print(sample_request['text'])#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9465507e-f258-472f-b1b6-f954ccc8145e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4291242052.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef process_individual_resume()\u001b[39m\n                                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef9ede-d5c5-4ad9-9047-b5a042aab92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce02ee-8440-49e0-9137-94d2777fcb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d1203-ee79-4c56-b8a5-c25cc90a730f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddb59ebf-1eb8-43ec-a20f-daf69c4c54af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1754646641'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(time.time()).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fa37a7-3a23-4fd1-aefc-ce17c369486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"postgresql+psycopg2://postgres:resume_db@localhost:5432/postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28346130-bdd5-410d-9510-d98207274eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_data = {'input_data': {'name': 'Ketan Kishore', 'resume_text': 'ketan.kishore31@gmail.com  +91 7488391342  House No. 4098, Sector   –   4/F,  Bokaro Steel City, Jharkhand   -  827004  Proficiencies  Generative AI/LLM   ★★★★  Machine Learning   ★★★★  Deep Learning   ★★★★  Py - Spark   ★★★★  Python   ★★★★  ML - Ops/LLM - Ops   ★★★  SQL   ★★★  Flask   ★★★  Tableau   ★★  Docker   ★★  NoSQL / Mongo DB   ★★  Web Crawling   ★★  Automation   ★★  Adobe Analytics   ★★  Ketan Kishore  An enthusiastic & high energy driven professional ,   targeting challenging assignments as a   Data  Scientist   with an organization of high repute .  GitHub Repo:   https://github.com/ketankishore27  Docker Hub Repo:   https://hub.docker.com/u/ketankishore27  LinkedIn :   https://www.linkedin.com/in/ketan - kishore - b89643150/  PROFILE SUMMARY  •   Goal - oriented professional with experience in   Insurance ,   Banking , and   Telecom   domains.  •   Skilled in   Predictive Modeling   using   Supervised   and   Unsupervised   Learning  •   E xperienced in   Text Analytics   using   NLP , LLM   and   Generative AI  •   Exposure   to   Cloud services like   AWS   and   Azure  •   Proficient   in working with   Distributed Framework   for   scalable   Analytics and Modelling  PROJECTS  T - Systems India Pvt Ltd   Apr’21   –   Present  Client:   Deutsche Telekom (Germany)  Project :   Common Service Desk  Tools: Open - AI ,   GPT, Text Embeddings, Prompt Engineering, Docker, Flask,   Evaluation,  Analytics /Troubleshooting , Git, Python  •   Integrated OpenAI   backend   to Service Desk chatbot  •   Designed and implemented Agentic architecture   alongside   R etrieval   A ugmented   G eneration .  •   Fine - tuned model responses   for   f low retrieval using text embeddings   to enhance accuracy .  •   Created   APIs   for real - time AI model integration .  •   Analysis for   incorrect responses from the chatbot.  •   Partnered with stakeholders to define clear project goals and success criteria .  Client:   Deutsche   Telekom   ( Germany,   Croatia , Hungary , Poland )  Project :   Broadband  Tools: Py - Spark, Machine Learning, Deep Learning,   SQL,   Airflow   , Analytics, Visualization, Python  •   Feature store for telecom data .  •   Developed   models to predict   issues   like   Device, I nstallation , Line, Wi - Fi   error etc.  •   D ashboards   for the bootstrapped new router   developed by   organization .  •   Created flows to   troubleshoot and find new issues in new router versions.  •   Developed customer profiling and journey analysis for people visiting our app.  •   Helped in finding out issues in the Deutsche Telecom `My Magenta` app  Bajaj Finance   Jun ’ 20   –   Apr’21  Client: Loans /Lending   Team  Project:   Money Manager  Tools: Spacy, Recurrent Neural Network, Python , Regex, AWS (Lambda, EC2, S3, Redshift, Boto3)  •   Created ML model to   classify   transactional message   and store entities  •   Assisting   in creating real time offer generation pipeline based on above extracted entities  Client: E - Store   Team  Project:   Nearest Dealer Solution  Tools: Machine Learning, Mongo - DB, Flask, Docker, Python  •   Created a   ML model to identify   nearest dealers   for   visitors   on the website.  •   Incorporated business rules to recommend dealers based on loyalty, reviews/score and   distance\\nEDUCATION  10th from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2010 with  95% GPA  12 th   from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2013 with  82% GPA  B .Tech. in Electronics and  Communication from SRM  University, Chennai in 2017  with 76.85 GPA  M.Tech in Data Science and  Engineering from BITS   -  Work Integrated, 2023 with  7.27 CGPA  PERSONAL DETAILS  Date of Birth: 20 th   February 1995  Lan guages Known: English and Hindi  HOBBIES  •   Travel  •   Cook  •   Gym  •   Personal Projects ( Here )  •   Friends Catchup  Client: Marketing   + Cards Team  Project:   Clickstream Analytics  Tools: Adobe Analytics, Py - Spark, Analytics,   Visualization, Reporting  •   Created reports on   Adobe Analytics   on Customer Journey, Path, Churn and Anomaly  •   A nalyze and   r ecommend concordan t /discordan t   simulations sent to the identified visitors  Capgemini.   Oct ’ 17   -   Jun ’20  Client:   Swiss Re  Project:   Trip Optimization   -   POC  Tools:   Tableau, Statistical /Constraint   Modelling , Python, Analytics  •   Developed a constrained   algorithm   to optimize trip allocation   cost .  •   Implemented hierarchical de - allocation of trips consi dering corporate band and real time  Tableau frontend filters  •   Created and presented   dashboard having   Drill Down Reports   and overall summary  Client:   Sunlife   Financials  Project:   News - Feed   –   POC  Tools:   N atural   L anguage   P rocessing ,   Convolution Neural Networks,   Web Scraping , Selenium,  Python, Flask, Docker, Analytics, Visualization  •   Developed a web crawler   to search   the web   and perform NLP task to create insights.  •   Worked   on possibility to use   satellite imagery to predict Catastrophes damage index .  Client:   Assurant Employee Benefits  Project: Anomaly and Churn Prediction  Project:   Analytics, Visualization,   Python, Basic Machine Learning  •   Analyze ,   Visualize,   and deduce KPI based on claims distributed over geographical area.  •   Implemented ML and DL model to predict   Fraudulent/Incorrect C laims  •   Work ed   parallel   on   development of   Churn Model   ( POC ).  •   Created   Python API’s and Selenium automations\\n', 'job_role': 'Data Science, AI Engineer'}, 'getContacts': {'mobile_number': '+91 7488391342', 'email_id': 'ketan.kishore31@gmail.com', 'color': 'green', 'comment': 'Both contact number and email ID are present.'}, 'getCustomScores': {'searchibility_score': 85, 'hard_skills_score': 90, 'soft_skill_score': 75, 'formatting_score': 70}, 'getSummaryOverview': {'score': 90, 'color': 'green', 'label': 'good', 'comment': 'The summary is highly relevant to the Data Science, AI Engineer role, mentioning key skills and tools like NLP, LLM, and Generative AI, and experience in relevant industries.', 'summary': ['Goal-oriented professional with experience in Insurance, Banking, and Telecom domains.', 'Skilled in Predictive Modeling using Supervised and Unsupervised Learning.', 'Experienced in Text Analytics using NLP, LLM, and Generative AI.', 'Exposure to Cloud services like AWS and Azure.', 'Proficient in working with Distributed Framework for scalable Analytics and Modelling.']}, 'getFunctionalConstituent': {'constituent': {'Telecom': '40%', 'Banking': '30%', 'Insurance': '20%', 'Finance': '10%'}, 'industries': ['Telecom', 'Banking', 'Insurance', 'Finance'], 'has_industry_experience': True, 'has_completed_college': True}, 'getOtherComments': {'headings_feedback': 'The resume includes key sections like Profile Summary, Projects, and Education, but lacks a dedicated Skills section; headings are not clearly labeled, making it difficult to navigate.', 'title_match': 'The resume includes relevant job titles such as Data Scientist and projects related to AI and Data Science, which align well with the target role.', 'formatting_feedback': 'The formatting is inconsistent with irregular spacing and alignment issues; bullet points and dates are not uniformly presented, affecting readability.'}, 'getEducation': [{'degree': 'B.Tech in Electronics and Communication', 'institution': 'SRM University, Chennai', 'start_year': 2013, 'end_year': 2017}, {'degree': 'M.Tech in Data Science and Engineering', 'institution': 'BITS - Work Integrated', 'start_year': 2021, 'end_year': 2023}], 'scoreResume': {'score': '85%', 'items': ['Add measurable achievements and specific metrics to project descriptions to highlight impact.', 'Ensure consistent formatting and alignment, especially in the contact information section.', 'Use consistent verb tenses throughout the resume for clarity and professionalism.', 'Consider removing less relevant skills or tools with lower proficiency to focus on strengths.', 'Reorganize the resume to prioritize the most relevant experience and skills for the Data Science, AI Engineer role.', 'Clarify vague descriptions by providing more context or examples of tasks and outcomes.']}, 'getTechnicalConstituent': {'high': ['Python', 'Machine Learning', 'Deep Learning', 'PySpark', 'Flask', 'Docker', 'SQL', 'NLP', 'AWS'], 'medium': ['Azure', 'Tableau', 'MongoDB', 'Adobe Analytics', 'Selenium', 'Airflow'], 'low': ['RNN', 'CNN', 'Web Scraping', 'Regex', 'Boto3', 'Redshift', 'EC2', 'S3']}, 'getCompany': [{'company': 'Capgemini', 'position': 'Data Scientist', 'start_year': 2017, 'end_year': 2020, 'employment_type': 'Permanent'}, {'company': 'Bajaj Finance', 'position': 'Data Scientist', 'start_year': 2020, 'end_year': 2021, 'employment_type': 'Permanent'}, {'company': 'T-Systems India Pvt Ltd', 'position': 'Data Scientist', 'start_year': 2021, 'end_year': 'Currently Working', 'employment_type': 'Permanent'}], 'getProjects': [{'title': 'Common Service Desk', 'description': 'Integrated OpenAI backend to Service Desk chatbot', 'technologies': ['Open-AI', 'GPT', 'Text Embeddings', 'Prompt Engineering', 'Docker', 'Flask', 'Python'], 'score': 90, 'color': 'light green', 'comment': 'Strong alignment with AI Engineer role due to use of OpenAI, GPT, and text embeddings.', 'stage': 'Production'}, {'title': 'Broadband', 'description': 'Developed models to predict issues like Device, Installation, Line, Wi-Fi error etc.', 'technologies': ['Py-Spark', 'Machine Learning', 'Deep Learning', 'SQL', 'Airflow', 'Python'], 'score': 85, 'color': 'light green', 'comment': 'High relevance due to use of ML and DL in a real-world telecom context.', 'stage': 'Production'}, {'title': 'Money Manager', 'description': 'Created ML model to classify transactional message and store entities', 'technologies': ['Spacy', 'Recurrent Neural Network', 'Python', 'AWS'], 'score': 80, 'color': 'light green', 'comment': 'Relevant due to ML model development and use of AWS, aligning with data science tasks.', 'stage': 'Production'}, {'title': 'Nearest Dealer Solution', 'description': 'Created a ML model to identify nearest dealers for visitors on the website.', 'technologies': ['Machine Learning', 'Mongo-DB', 'Flask', 'Docker', 'Python'], 'score': 75, 'color': 'light orange', 'comment': 'Good use of ML and relevant technologies, though less directly related to AI-specific tasks.', 'stage': 'Production'}, {'title': 'Clickstream Analytics', 'description': 'Created reports on Adobe Analytics on Customer Journey, Path, Churn and Anomaly', 'technologies': ['Adobe Analytics', 'Py-Spark', 'Analytics', 'Visualization'], 'score': 70, 'color': 'light orange', 'comment': 'Relevant for data analysis but less focus on AI-specific technologies.', 'stage': 'Production'}, {'title': 'Trip Optimization - POC', 'description': 'Developed a constrained algorithm to optimize trip allocation cost.', 'technologies': ['Tableau', 'Statistical/Constraint Modelling', 'Python', 'Analytics'], 'score': 65, 'color': 'light orange', 'comment': 'Involves optimization and analytics, relevant but less focus on AI/ML.', 'stage': 'POC'}, {'title': 'News-Feed – POC', 'description': 'Developed a web crawler to search the web and perform NLP task to create insights.', 'technologies': ['Natural Language Processing', 'Convolution Neural Networks', 'Web Scraping', 'Selenium', 'Python', 'Flask', 'Docker'], 'score': 85, 'color': 'light green', 'comment': 'Strong alignment with AI Engineer role due to NLP and CNN usage.', 'stage': 'POC'}, {'title': 'Anomaly and Churn Prediction', 'description': 'Implemented ML and DL model to predict Fraudulent/Incorrect Claims', 'technologies': ['Python', 'Basic Machine Learning'], 'score': 80, 'color': 'light green', 'comment': 'Relevant due to ML/DL model development for prediction tasks.', 'stage': 'Production'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d479a-25bf-4f98-81e4-8a2c7698e1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2481fe45-07ea-4d60-8c30-48f1572d89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(assembled_field: dict):\n",
    "    print(\"Got request to assmble data\")\n",
    "\n",
    "    db_colNames = ['candidate_id', 'name', 'job_role', 'resume_raw_text', 'score_resume', 'get_contacts', 'get_summary_overview', 'get_custom_scores', \n",
    "                   'get_other_comments', 'get_functional_constituent', 'get_technical_constituent', 'get_education', 'get_projects', \n",
    "                   'get_company']\n",
    "    \n",
    "    json_cols = ['get_contacts', 'get_custom_scores', 'get_summary_overview', 'get_functional_constituent', 'get_other_comments', 'get_education', 'score_resume', 'get_technical_constituent', 'get_company', 'get_projects']\n",
    "    \n",
    "    col_mapping = {i: JSON for i in json_cols}\n",
    "    col_mapping.update({\n",
    "        'candidate_id': Text,\n",
    "        'name': Text,\n",
    "        'job_role': Text,\n",
    "        'resume_raw_text': Text\n",
    "    })\n",
    "    \n",
    "    candidate_id = f\"Candidate-{str(time.time()).split('.')[0]}\"\n",
    "    name = assembled_field.get(\"input_data\", None).get(\"name\", None)\n",
    "    job_role = assembled_field.get(\"input_data\", None).get(\"job_role\", None)\n",
    "    resume_text = assembled_field.get(\"input_data\", None).get(\"resume_text\", None)\n",
    "    \n",
    "    if any(ent is None for ent in [name, job_role, resume_text]):\n",
    "        return {\"response\": \"Name/Job-Role/Resume cant be None\"}\n",
    "        \n",
    "    getContacts = assembled_field.get(\"getContacts\", None)\n",
    "    getCustomScores = assembled_field.get(\"getCustomScores\", None)\n",
    "    getSummaryOverview = assembled_field.get(\"getSummaryOverview\", None)\n",
    "    getFunctionalConstituent = assembled_field.get(\"getFunctionalConstituent\", None)\n",
    "    getOtherComments = assembled_field.get(\"getOtherComments\", None)\n",
    "    getEducation = assembled_field.get(\"getEducation\", None)\n",
    "    scoreResume = assembled_field.get(\"scoreResume\", None)\n",
    "    getTechnicalConstituent = assembled_field.get(\"getTechnicalConstituent\", None)\n",
    "    getCompany = assembled_field.get(\"getCompany\", None)\n",
    "    getProjects = assembled_field.get(\"getProjects\", None)\n",
    "\n",
    "    data = pd.DataFrame([[candidate_id, name, job_role, resume_text, scoreResume, getContacts, getSummaryOverview, getCustomScores, getOtherComments, getFunctionalConstituent, \n",
    "                          getTechnicalConstituent, getEducation, getProjects, getCompany]], \n",
    "                       columns = db_colNames)\n",
    "    \n",
    "    print(data.columns)\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        data.to_sql(name=\"resume_store\", con=conn, if_exists=\"append\", index=False, dtype=col_mapping)\n",
    "        \n",
    "    return {\"response\": \"Data inserted successfully\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e757400e-43da-4232-a31c-c51946411075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got request to assmble data\n",
      "Index(['candidate_id', 'name', 'job_role', 'resume_raw_text', 'score_resume',\n",
      "       'get_contacts', 'get_summary_overview', 'get_custom_scores',\n",
      "       'get_other_comments', 'get_functional_constituent',\n",
      "       'get_technical_constituent', 'get_education', 'get_projects',\n",
      "       'get_company'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'Data inserted successfully'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_data(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8704a1-c19d-4621-b0c7-5ca6a5818f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fccb027-4638-44b1-8288-6fa242af3021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_data', 'getContacts', 'getCustomScores', 'getSummaryOverview', 'getFunctionalConstituent', 'getOtherComments', 'getEducation', 'scoreResume', 'getTechnicalConstituent', 'getCompany', 'getProjects'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25562e8-cf15-4c07-8a46-f4ce4a87038b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'getCustomScores': {'searchibility_score': 85,\n",
       "  'hard_skills_score': 90,\n",
       "  'soft_skill_score': 75,\n",
       "  'formatting_score': 70}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02e6c-4d3e-4efd-ad1b-692e02fee58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
