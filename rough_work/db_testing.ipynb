{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9635b68d-375e-4335-90e4-f469cc2f488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import JSON, Text\n",
    "#from sqlalchemy.dialects.postgresql import JSON \n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30892e60-d0e7-47b2-8224-8bc1957bb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddb59ebf-1eb8-43ec-a20f-daf69c4c54af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1754646641'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(time.time()).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fa37a7-3a23-4fd1-aefc-ce17c369486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"postgresql+psycopg2://postgres:resume_db@localhost:5432/postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28346130-bdd5-410d-9510-d98207274eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_data = {'input_data': {'name': 'Ketan Kishore', 'resume_text': 'ketan.kishore31@gmail.com  +91 7488391342  House No. 4098, Sector   –   4/F,  Bokaro Steel City, Jharkhand   -  827004  Proficiencies  Generative AI/LLM   ★★★★  Machine Learning   ★★★★  Deep Learning   ★★★★  Py - Spark   ★★★★  Python   ★★★★  ML - Ops/LLM - Ops   ★★★  SQL   ★★★  Flask   ★★★  Tableau   ★★  Docker   ★★  NoSQL / Mongo DB   ★★  Web Crawling   ★★  Automation   ★★  Adobe Analytics   ★★  Ketan Kishore  An enthusiastic & high energy driven professional ,   targeting challenging assignments as a   Data  Scientist   with an organization of high repute .  GitHub Repo:   https://github.com/ketankishore27  Docker Hub Repo:   https://hub.docker.com/u/ketankishore27  LinkedIn :   https://www.linkedin.com/in/ketan - kishore - b89643150/  PROFILE SUMMARY  •   Goal - oriented professional with experience in   Insurance ,   Banking , and   Telecom   domains.  •   Skilled in   Predictive Modeling   using   Supervised   and   Unsupervised   Learning  •   E xperienced in   Text Analytics   using   NLP , LLM   and   Generative AI  •   Exposure   to   Cloud services like   AWS   and   Azure  •   Proficient   in working with   Distributed Framework   for   scalable   Analytics and Modelling  PROJECTS  T - Systems India Pvt Ltd   Apr’21   –   Present  Client:   Deutsche Telekom (Germany)  Project :   Common Service Desk  Tools: Open - AI ,   GPT, Text Embeddings, Prompt Engineering, Docker, Flask,   Evaluation,  Analytics /Troubleshooting , Git, Python  •   Integrated OpenAI   backend   to Service Desk chatbot  •   Designed and implemented Agentic architecture   alongside   R etrieval   A ugmented   G eneration .  •   Fine - tuned model responses   for   f low retrieval using text embeddings   to enhance accuracy .  •   Created   APIs   for real - time AI model integration .  •   Analysis for   incorrect responses from the chatbot.  •   Partnered with stakeholders to define clear project goals and success criteria .  Client:   Deutsche   Telekom   ( Germany,   Croatia , Hungary , Poland )  Project :   Broadband  Tools: Py - Spark, Machine Learning, Deep Learning,   SQL,   Airflow   , Analytics, Visualization, Python  •   Feature store for telecom data .  •   Developed   models to predict   issues   like   Device, I nstallation , Line, Wi - Fi   error etc.  •   D ashboards   for the bootstrapped new router   developed by   organization .  •   Created flows to   troubleshoot and find new issues in new router versions.  •   Developed customer profiling and journey analysis for people visiting our app.  •   Helped in finding out issues in the Deutsche Telecom `My Magenta` app  Bajaj Finance   Jun ’ 20   –   Apr’21  Client: Loans /Lending   Team  Project:   Money Manager  Tools: Spacy, Recurrent Neural Network, Python , Regex, AWS (Lambda, EC2, S3, Redshift, Boto3)  •   Created ML model to   classify   transactional message   and store entities  •   Assisting   in creating real time offer generation pipeline based on above extracted entities  Client: E - Store   Team  Project:   Nearest Dealer Solution  Tools: Machine Learning, Mongo - DB, Flask, Docker, Python  •   Created a   ML model to identify   nearest dealers   for   visitors   on the website.  •   Incorporated business rules to recommend dealers based on loyalty, reviews/score and   distance\\nEDUCATION  10th from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2010 with  95% GPA  12 th   from Delhi Public  School, Bokaro Steel City,  Jharkhand in 2013 with  82% GPA  B .Tech. in Electronics and  Communication from SRM  University, Chennai in 2017  with 76.85 GPA  M.Tech in Data Science and  Engineering from BITS   -  Work Integrated, 2023 with  7.27 CGPA  PERSONAL DETAILS  Date of Birth: 20 th   February 1995  Lan guages Known: English and Hindi  HOBBIES  •   Travel  •   Cook  •   Gym  •   Personal Projects ( Here )  •   Friends Catchup  Client: Marketing   + Cards Team  Project:   Clickstream Analytics  Tools: Adobe Analytics, Py - Spark, Analytics,   Visualization, Reporting  •   Created reports on   Adobe Analytics   on Customer Journey, Path, Churn and Anomaly  •   A nalyze and   r ecommend concordan t /discordan t   simulations sent to the identified visitors  Capgemini.   Oct ’ 17   -   Jun ’20  Client:   Swiss Re  Project:   Trip Optimization   -   POC  Tools:   Tableau, Statistical /Constraint   Modelling , Python, Analytics  •   Developed a constrained   algorithm   to optimize trip allocation   cost .  •   Implemented hierarchical de - allocation of trips consi dering corporate band and real time  Tableau frontend filters  •   Created and presented   dashboard having   Drill Down Reports   and overall summary  Client:   Sunlife   Financials  Project:   News - Feed   –   POC  Tools:   N atural   L anguage   P rocessing ,   Convolution Neural Networks,   Web Scraping , Selenium,  Python, Flask, Docker, Analytics, Visualization  •   Developed a web crawler   to search   the web   and perform NLP task to create insights.  •   Worked   on possibility to use   satellite imagery to predict Catastrophes damage index .  Client:   Assurant Employee Benefits  Project: Anomaly and Churn Prediction  Project:   Analytics, Visualization,   Python, Basic Machine Learning  •   Analyze ,   Visualize,   and deduce KPI based on claims distributed over geographical area.  •   Implemented ML and DL model to predict   Fraudulent/Incorrect C laims  •   Work ed   parallel   on   development of   Churn Model   ( POC ).  •   Created   Python API’s and Selenium automations\\n', 'job_role': 'Data Science, AI Engineer'}, 'getContacts': {'mobile_number': '+91 7488391342', 'email_id': 'ketan.kishore31@gmail.com', 'color': 'green', 'comment': 'Both contact number and email ID are present.'}, 'getCustomScores': {'searchibility_score': 85, 'hard_skills_score': 90, 'soft_skill_score': 75, 'formatting_score': 70}, 'getSummaryOverview': {'score': 90, 'color': 'green', 'label': 'good', 'comment': 'The summary is highly relevant to the Data Science, AI Engineer role, mentioning key skills and tools like NLP, LLM, and Generative AI, and experience in relevant industries.', 'summary': ['Goal-oriented professional with experience in Insurance, Banking, and Telecom domains.', 'Skilled in Predictive Modeling using Supervised and Unsupervised Learning.', 'Experienced in Text Analytics using NLP, LLM, and Generative AI.', 'Exposure to Cloud services like AWS and Azure.', 'Proficient in working with Distributed Framework for scalable Analytics and Modelling.']}, 'getFunctionalConstituent': {'constituent': {'Telecom': '40%', 'Banking': '30%', 'Insurance': '20%', 'Finance': '10%'}, 'industries': ['Telecom', 'Banking', 'Insurance', 'Finance'], 'has_industry_experience': True, 'has_completed_college': True}, 'getOtherComments': {'headings_feedback': 'The resume includes key sections like Profile Summary, Projects, and Education, but lacks a dedicated Skills section; headings are not clearly labeled, making it difficult to navigate.', 'title_match': 'The resume includes relevant job titles such as Data Scientist and projects related to AI and Data Science, which align well with the target role.', 'formatting_feedback': 'The formatting is inconsistent with irregular spacing and alignment issues; bullet points and dates are not uniformly presented, affecting readability.'}, 'getEducation': [{'degree': 'B.Tech in Electronics and Communication', 'institution': 'SRM University, Chennai', 'start_year': 2013, 'end_year': 2017}, {'degree': 'M.Tech in Data Science and Engineering', 'institution': 'BITS - Work Integrated', 'start_year': 2021, 'end_year': 2023}], 'scoreResume': {'score': '85%', 'items': ['Add measurable achievements and specific metrics to project descriptions to highlight impact.', 'Ensure consistent formatting and alignment, especially in the contact information section.', 'Use consistent verb tenses throughout the resume for clarity and professionalism.', 'Consider removing less relevant skills or tools with lower proficiency to focus on strengths.', 'Reorganize the resume to prioritize the most relevant experience and skills for the Data Science, AI Engineer role.', 'Clarify vague descriptions by providing more context or examples of tasks and outcomes.']}, 'getTechnicalConstituent': {'high': ['Python', 'Machine Learning', 'Deep Learning', 'PySpark', 'Flask', 'Docker', 'SQL', 'NLP', 'AWS'], 'medium': ['Azure', 'Tableau', 'MongoDB', 'Adobe Analytics', 'Selenium', 'Airflow'], 'low': ['RNN', 'CNN', 'Web Scraping', 'Regex', 'Boto3', 'Redshift', 'EC2', 'S3']}, 'getCompany': [{'company': 'Capgemini', 'position': 'Data Scientist', 'start_year': 2017, 'end_year': 2020, 'employment_type': 'Permanent'}, {'company': 'Bajaj Finance', 'position': 'Data Scientist', 'start_year': 2020, 'end_year': 2021, 'employment_type': 'Permanent'}, {'company': 'T-Systems India Pvt Ltd', 'position': 'Data Scientist', 'start_year': 2021, 'end_year': 'Currently Working', 'employment_type': 'Permanent'}], 'getProjects': [{'title': 'Common Service Desk', 'description': 'Integrated OpenAI backend to Service Desk chatbot', 'technologies': ['Open-AI', 'GPT', 'Text Embeddings', 'Prompt Engineering', 'Docker', 'Flask', 'Python'], 'score': 90, 'color': 'light green', 'comment': 'Strong alignment with AI Engineer role due to use of OpenAI, GPT, and text embeddings.', 'stage': 'Production'}, {'title': 'Broadband', 'description': 'Developed models to predict issues like Device, Installation, Line, Wi-Fi error etc.', 'technologies': ['Py-Spark', 'Machine Learning', 'Deep Learning', 'SQL', 'Airflow', 'Python'], 'score': 85, 'color': 'light green', 'comment': 'High relevance due to use of ML and DL in a real-world telecom context.', 'stage': 'Production'}, {'title': 'Money Manager', 'description': 'Created ML model to classify transactional message and store entities', 'technologies': ['Spacy', 'Recurrent Neural Network', 'Python', 'AWS'], 'score': 80, 'color': 'light green', 'comment': 'Relevant due to ML model development and use of AWS, aligning with data science tasks.', 'stage': 'Production'}, {'title': 'Nearest Dealer Solution', 'description': 'Created a ML model to identify nearest dealers for visitors on the website.', 'technologies': ['Machine Learning', 'Mongo-DB', 'Flask', 'Docker', 'Python'], 'score': 75, 'color': 'light orange', 'comment': 'Good use of ML and relevant technologies, though less directly related to AI-specific tasks.', 'stage': 'Production'}, {'title': 'Clickstream Analytics', 'description': 'Created reports on Adobe Analytics on Customer Journey, Path, Churn and Anomaly', 'technologies': ['Adobe Analytics', 'Py-Spark', 'Analytics', 'Visualization'], 'score': 70, 'color': 'light orange', 'comment': 'Relevant for data analysis but less focus on AI-specific technologies.', 'stage': 'Production'}, {'title': 'Trip Optimization - POC', 'description': 'Developed a constrained algorithm to optimize trip allocation cost.', 'technologies': ['Tableau', 'Statistical/Constraint Modelling', 'Python', 'Analytics'], 'score': 65, 'color': 'light orange', 'comment': 'Involves optimization and analytics, relevant but less focus on AI/ML.', 'stage': 'POC'}, {'title': 'News-Feed – POC', 'description': 'Developed a web crawler to search the web and perform NLP task to create insights.', 'technologies': ['Natural Language Processing', 'Convolution Neural Networks', 'Web Scraping', 'Selenium', 'Python', 'Flask', 'Docker'], 'score': 85, 'color': 'light green', 'comment': 'Strong alignment with AI Engineer role due to NLP and CNN usage.', 'stage': 'POC'}, {'title': 'Anomaly and Churn Prediction', 'description': 'Implemented ML and DL model to predict Fraudulent/Incorrect Claims', 'technologies': ['Python', 'Basic Machine Learning'], 'score': 80, 'color': 'light green', 'comment': 'Relevant due to ML/DL model development for prediction tasks.', 'stage': 'Production'}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d479a-25bf-4f98-81e4-8a2c7698e1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2481fe45-07ea-4d60-8c30-48f1572d89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(assembled_field: dict):\n",
    "    print(\"Got request to assmble data\")\n",
    "\n",
    "    db_colNames = ['candidate_id', 'name', 'job_role', 'resume_raw_text', 'score_resume', 'get_contacts', 'get_summary_overview', 'get_custom_scores', \n",
    "                   'get_other_comments', 'get_functional_constituent', 'get_technical_constituent', 'get_education', 'get_projects', \n",
    "                   'get_company']\n",
    "    \n",
    "    json_cols = ['get_contacts', 'get_custom_scores', 'get_summary_overview', 'get_functional_constituent', 'get_other_comments', 'get_education', 'score_resume', 'get_technical_constituent', 'get_company', 'get_projects']\n",
    "    \n",
    "    col_mapping = {i: JSON for i in json_cols}\n",
    "    col_mapping.update({\n",
    "        'candidate_id': Text,\n",
    "        'name': Text,\n",
    "        'job_role': Text,\n",
    "        'resume_raw_text': Text\n",
    "    })\n",
    "    \n",
    "    candidate_id = f\"Candidate-{str(time.time()).split('.')[0]}\"\n",
    "    name = assembled_field.get(\"input_data\", None).get(\"name\", None)\n",
    "    job_role = assembled_field.get(\"input_data\", None).get(\"job_role\", None)\n",
    "    resume_text = assembled_field.get(\"input_data\", None).get(\"resume_text\", None)\n",
    "    \n",
    "    if any(ent is None for ent in [name, job_role, resume_text]):\n",
    "        return {\"response\": \"Name/Job-Role/Resume cant be None\"}\n",
    "        \n",
    "    getContacts = assembled_field.get(\"getContacts\", None)\n",
    "    getCustomScores = assembled_field.get(\"getCustomScores\", None)\n",
    "    getSummaryOverview = assembled_field.get(\"getSummaryOverview\", None)\n",
    "    getFunctionalConstituent = assembled_field.get(\"getFunctionalConstituent\", None)\n",
    "    getOtherComments = assembled_field.get(\"getOtherComments\", None)\n",
    "    getEducation = assembled_field.get(\"getEducation\", None)\n",
    "    scoreResume = assembled_field.get(\"scoreResume\", None)\n",
    "    getTechnicalConstituent = assembled_field.get(\"getTechnicalConstituent\", None)\n",
    "    getCompany = assembled_field.get(\"getCompany\", None)\n",
    "    getProjects = assembled_field.get(\"getProjects\", None)\n",
    "\n",
    "    data = pd.DataFrame([[candidate_id, name, job_role, resume_text, scoreResume, getContacts, getSummaryOverview, getCustomScores, getOtherComments, getFunctionalConstituent, \n",
    "                          getTechnicalConstituent, getEducation, getProjects, getCompany]], \n",
    "                       columns = db_colNames)\n",
    "    \n",
    "    print(data.columns)\n",
    "    \n",
    "    with engine.begin() as conn:\n",
    "        data.to_sql(name=\"resume_store\", con=conn, if_exists=\"append\", index=False, dtype=col_mapping)\n",
    "        \n",
    "    return {\"response\": \"Data inserted successfully\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e757400e-43da-4232-a31c-c51946411075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got request to assmble data\n",
      "Index(['candidate_id', 'name', 'job_role', 'resume_raw_text', 'score_resume',\n",
      "       'get_contacts', 'get_summary_overview', 'get_custom_scores',\n",
      "       'get_other_comments', 'get_functional_constituent',\n",
      "       'get_technical_constituent', 'get_education', 'get_projects',\n",
      "       'get_company'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'Data inserted successfully'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_data(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8704a1-c19d-4621-b0c7-5ca6a5818f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-DeutscheTelekomAG/Projects/COE_Projects/CareerDevelopmentTool/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fccb027-4638-44b1-8288-6fa242af3021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_data', 'getContacts', 'getCustomScores', 'getSummaryOverview', 'getFunctionalConstituent', 'getOtherComments', 'getEducation', 'scoreResume', 'getTechnicalConstituent', 'getCompany', 'getProjects'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25562e8-cf15-4c07-8a46-f4ce4a87038b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'getCustomScores': {'searchibility_score': 85,\n",
       "  'hard_skills_score': 90,\n",
       "  'soft_skill_score': 75,\n",
       "  'formatting_score': 70}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e02e6c-4d3e-4efd-ad1b-692e02fee58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
